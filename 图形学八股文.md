---

typora-copy-images-to: image
typora-root-url: image
---

# 图形学八股文

# 一、数学运算

## 1、**判断点在三角形内**

重心坐标、叉乘

## 2、**判断点在矩形内**

点乘。给定p1、p2、p3、p4和p。 只需要保证 对角的两点p1p3。存在dot(p1p2, p1p) > 0、dot(p1p4,p1p) > 0

类似于p3。就能断定点在矩形内。

## 3、**给定点集判断凹凸**

叉乘，点abc，利用bc x ba > 0 即内角小于180.

## 4、**给定入射光线和法线，求反射光线**

```c++
假设光线从表面指向光源 Wi, 法向量n
Wi = 2dot(Wi,n)*n - Wi
```

## 5、**叉乘求多边形面积**

划分成多个三角形，利用三角形面积公式 S = 1/2 AB x AC

无需判断凹凸，结果一定正确。只需要按顺序即可 SOAB, SOBC, SOCD.....

## 6、**空间求两直线距离**

直线1表示方式 过A点，方向n1,  直线2表示方式过B点，方向n2

既有n1xn2 垂直n1，n2. 同时dot(AB, n1 x n2)  / ||n1 x n2||  即AB在垂线的投影，需要除以长度，因为垂线不是单位长度。

## 7、**空间点到三角形的最近距离**

叉乘，算出垂直三角形的法向量n， 空间一点p - p'= n, 反算出点p在面上的投影点p'

## 8、**行列式为0的意义**

不满秩，向量线性相关

## 9、**判断光线和球相交**

```c++
// 给定光线 p = o + dt，球体方程 (p - center)^2 = r * r
//联立方程可得 (dt + o - center) ^ 2 = r * r 
// 化简得  d*d t^2 + 2d(o - center)t + (o - center)^2 - r * r = 0;
//求解t,  a = dot(d,d), b = 2 dot(d, o - center), c = dot(o - center, o - center) - r * r;
利用求根公式，求解即可
```

优化的方式

![image-20220619173251605](image-20220619173251605.png)





**判断光线和三角形相交**

1、先判断光线和三角形所在的面是否相交，求出交点，然后判断交点是否在三角形内

2、利用重心坐标表示出交点，然后联立方程，用克拉默法则求解， 最后判断重心坐标都大于0，且t也大于0



**判断两个三角形相交**

答：转化为光线和三角形相交，依次判断每条边的光线和三角形是否相交，求出交点，判断交点是否在边的线段内。





## 10、**如何在圆内随机生成一个点**

```c++
default_random_engine e; //随机数引擎
uniform_real_distribution<double> u(0, 1); //随机分布函数
double r = u(e);
```

![image-20220618204936452](image-20220618204936452.png)

## 11、计算一个图片的大小

如果是1024 * 1024

对于每个像素采样rgba，且位深是8bit的时候，32位，表示256种颜色，1024 * 1024 * 4

当然也可以有8位，16为，24位等。



## 12、点和任意多边形判断内外

点朝任意方向发射一条射线，求出射线和多边形之间的交点个数，如果奇数就在内，如果偶数就在外。



# 二、图形学

### 1、齐次坐标

齐次坐标将向量提升了一个维度，引入主要是为了解决空间中一个三维点无法用3x3的矩阵表达平移的问题，同时空间中的两条平行线在最远处是会相交的，透视投影，而利用齐次坐标就能表示。=在最远处的w分量为0，即表示无穷远。利用w是否为0判断是点还是向量。

### 2、法线贴图，TBN矩阵

作用：在图形模型面数不多的情况下，运用一个精雕的法线贴图，使得呈现出非常逼真的效果。

要点：

1、存储在切线空间，可以应对模型转变后，法线对应不上模型的情况。 只需要在使用的时候使用TBN矩阵转到世界空间。 

2、蓝色是因为存储在切线空间的贴图，大多指向正z方向，所以偏蓝

3、计算的时候技巧，在顶点着色器中将TBN矩阵左乘model矩阵，逆转置，到世界坐标，然后利用TBN到像素着色器中，将采样的法线左乘TBN矩阵变到世界坐标，计算光照。  也可以将光源，视点，着色点转到切线空间，左乘TBN的逆矩阵(转置)，这样不用每个像素都乘一遍TBN矩阵，增加了效率。

计算TBN矩阵：根据三角形面，其中两条边的向量，和uv向量，使用u，v长度差和T,B的基向量 来表示边的向量。

​	

### 3、裁剪空间

从观察空间通过投影变换到裁剪空间，此时定义有6个面组成的视椎体，将在视椎体外的丢弃，内部的保留，在边界的部分进行裁剪。

裁剪的规则，判断三角形的每个点和面的关系，利用向量的面的法向量点乘，得到点在面的内侧还是外侧，然后进一步判断出线跟面的关系，对于两个点在面不同侧的情况，进行裁剪，得到交点重新组装成新的三角形。 

一般来说 在裁剪空间进行裁剪，优点是裁剪空间的6个面表示简单，都是-w到w，其次裁剪空间下的插值为线性插值，在NDC空间下是经过压缩的。

### 4、视椎体

fov视场角，屏幕宽高比，zNear，Zfar



### 5、Model矩阵

描述空间物体的的变换，使用顺序是缩放，旋转，平移。 缩放要在旋转之前，平移要在旋转之后。 结果更加符合人的直观。

给定M矩阵，能够分解出平移矩阵

然后根据旋转矩阵和缩放矩阵的乘积，可知某一列的平方和的根就是缩放的比例大小，求出缩放后，每列除以缩放比例，就得到旋转矩阵



给定任意旋转轴a，待旋转列向量为r，求饶a旋转的公式

1、构建以a为x轴的坐标系，任取不与a重合的t，叉乘计算出bc

2、以A为[abc]，则可知Ar' = r, r'为r向量在新坐标系的表示

3、r' = A-1r,   此时转换 到新坐标下，然后绕x轴旋转公式， Rr' = RA-1r

4、最后将结果转到原坐标  ARr' = ARA-1r，   旋转矩阵为ARA-1

### 6、View 矩阵

视图矩阵，给定eye,target, up，推导相应的view mat

```c++
vec3 D = (eye - target).normal  //z轴
vec3 R = cross(up, D);
vec3 U = cross(D, R);
RUD的逆矩阵，即转置
 				 [Rx, Ry, Rz, 0]	[1, 0, 0, -eye_x]
   view_space =	 [Ux, Uy, Uz, 0] *  [0, 1, 0, -eye_y]
    			 [Dx, Dy, Dz, 0]	[0, 0, 1, -eye_z]
    	         [0,  0,   0, 1]    [0, 0, 0,  1    ]
```

### 7、Projection矩阵

分为正交投影和透视投影，区别是透视透明达到了平行线在远处会相交的性质。

正交投影

```c++
mat4 ortho_mat(float left, float right, float buttom, float top, float znear, float zfar) 
{
}
```

![image-20220621150934884](image-20220621150934884.png)

透视投影

```
mat4 perspective(float fov, float aspect, float zNear, float zFar)
```

![image-20220621151210361](image-20220621151210361.png)

### 8、视口变换

将坐标从-1到1的NDC空间，变成屏幕空间，从三维到二维，原本的z信息存到深度缓存中。 一般来说存储的是非线性深度，即近平面的z的精度大，远处的精度小。详细见zbuffer

### 9、欧拉角和四元数

**欧拉角**：定义了三个围绕坐标轴的转动，俯仰角pitch,偏航角yaw，滚动角roll

优点：很直观，单个维度上方便插值

缺点：不能进行任意方向的插值

导致万向节死锁，旋转的次序对结果有影响。比如按照zxy的顺序，此时将x旋转90度，y轴和z轴重合，丢失了y的旋转自由度(因为z是第一优先级，所以将不会旋转90的当做中间轴，或者控制不会达到90度)

**矩阵：**

优点：不受万向节死锁影响，独一无二的表达任意旋转，对于纯旋转，通过逆矩阵，通过求转置很方便求。0

缺点：表达方式不直观，不容易计算插值，需要大量存储空间

**四元数：**

表示方式q = [qv qs] = [asinθ / 2, cosθ / 2];    qv表示旋转向量，qs表示标量，a表示旋转轴，

分别是旋转轴*旋转半角的正弦，第四个是旋转半角的余弦，根据右手定则表示旋转

q* = [-qv, qs]， q-1 = q* / |q|^2

对于单位长度的四元数，逆等于共轭， 四元数的乘法使用**格拉斯曼积**

如何用四元数旋转矢量？  将矢量部分变成四元数，加个标量的0，然后使用旋转公式

计算向量的旋转方法： v' = qvq(-1)， 当时单位长度时，使用 v' = qvq*



优点：

1、相对于矩阵，更少的存储空间，更快的运算方式

2、方便计算插值

线性插值：对于qa,qb之间的任意时间t 插值 q  = normalize((1 - t)qa + tqb)

球面线性插值：线性插值只考虑了球面上沿着弦的插值，导致了以t的恒定速度改变时，角度的变化并不恒定，两边慢，中间快。所以使用球面插值

q = (sin(1 - t)θ/sinθ) qa + (sintθ/sinθ)qb

3、解决了万向节死锁问题

### 10、Gamma矫正

对于一般的CRT显示器，输出的亮度 = 电压的Gamma次幂，Gamma通常为2.2,  因此当我们计算好亮度后，显示在显示器上，会比我们期望的亮度更暗，所以需要进行Gamma矫正，通过1/Gamma 次方。

此外，一般来说纹理的创建者，在创建diffuse和AO贴图时，都在srgb空间，这个空间下图像已经进行过gamma矫正了，所以显示的图像不需要再gamma矫正，但是在进行计算的时候，需要先转到线性空间，gamma次方，计算完后，再进行gamma矫正， 否则显示不正确。

### 11、后处理

#### 1、色调映射(Tone mapping)

场景中的亮度很可能超过1.0，这样就会被截断，从而损失细节，我们可以将HDR的图像结果无损转换到LDR中，通过色调映射

1、Reinhard，  result = hdrColor / (hdrColor + vec3(1.0));  将亮的区域整体变暗，同时暗的区域也会变的纹理不清晰

2、应用曝光参数。 在白天使用低曝光，晚上使用高曝光 vec3 mapped = vec3(1.0) - exp(-hdrColor * exposure);

3、ACES 

```c++
float float_aces(float value) {
    float a = 2.51f;
    float b = 0.03f;
    float c = 2.43f;
    float d = 0.59f;
    float e = 0.14f;
    value = (value * (a * value + b)) / (value * (c * value + d) + e);
    return float_saturate(value);
}
```



#### 2、泛光(Bloom)

将场景中的高亮部分计算出来，给RGB不同的权重(G最高)，求个加权和，将计算出来的结果跟平均光场亮度比较； 然后将高亮图像进行降采样，在最低层使用一个高斯模糊，然后一层层放大，每次都和当前降采样的图进行加权平均，一直到最原始的图。

高斯模糊快速实现---将二维转成两个一维的，降低时间复杂度

![image-20220726160722362](image-20220726160722362.png)

![image-20220705203744158](image-20220705203744158.png)

#### 3、Color Grading

使用LUT，将原始颜色和目标颜色做一个映射， 将256x256x256的压缩到32x32x32， 然后使用插值计算相应的值即可。



### 12、alpha混合

实现半透明物体的一种方法，在片段着色器运行完，且所有测试都通过之后， 通过混合方程将当前片段颜色和颜色缓冲中的值进行混合

注意点：需要先渲染不透明物体，然后按照顺序，从后往前绘制透明物体。 如果先绘制前面的透明物体，因为zbuffer的关系，后面的物体就不会被渲染，就不会产生透明的效果。



### 13、提高渲染效率，稳定帧率

- 视椎体裁剪，利用aabb包围盒，计算AABB和视椎体相交，不相交的直接pass
- 利用空间加速结构，八叉树，BVH等，管理场景，加速一些场景交互，不用去遍历所有物体，只需要遍历相对应的子结构内即可。
- LOD，比如曲面细分，在近处看得清的地方，用精细模型，在远处就用粗略模型。
- GPU优化 

### 14、利用2d坐标、深度信息和projection矩阵，如何获取3d坐标

构建坐标点 (x, y, z, 1)，左乘Projection逆矩阵，最后除以w

### 15、次表面散射

描述光线进入透明或者半透明物体后，经过多次散射从另一个点射出产生的效果。 主要用于皮肤、玉石、蜡烛等。

实现方式：基于深度贴图的方式，渲染一张阴影贴图，用于记录光线需要经过多少距离才能通过遮挡物，然后计算的时候加入漫反射光照，光照强度和光线光线透过物体的距离成反比，这样对比比较薄的表面，就会产生淡淡泛光。



### 16、反射算法SSR

屏幕空间反射算法，一种常见的实现反射的方法。原理是对于每个像素点，发出一条射线，求出在平面上的反射光线，并且按照步进的方式采样，将采样点转化到屏幕空间，和屏幕上的像素做深度比较，一旦小于某个阈值就停止采样。

如何确定步长？

通过深度图的mipmap，将场景的深度图，按照每四个像素合一个的方式，合并规则是取四个里面深度值最小的，直到生成所有的mipmap。 然后每次步进从1开始，如果没有相交，就到上一级的mipmap中，再走一格(也就是底层的两格)，如果没有就继续加大步长，如果相交就判断交点，去下一级上判断，直到最原始的一层上，找到交点，取它的颜色作为radiance添加到反射方程中。

缺点：只能做屏幕上的反射，对于屏幕外的物体无能为力，资源消耗比较大，粗糙物体不好处理。



### 17、LOD

根据模型距离摄像机的距离，选择不同精细程度的模型，摄像机离得越近模型精度越高。

优点：提高渲染效率，优化性能。 缺点：增加了内存消耗。



### 18、贴花的实现

将贴花设置为长方形区域，按照某个方向投影，在投影方向的表面第一次相交的地方成为贴花的面片，然后从中提取出三角形，使用相应的矩形包围面裁剪三角形，生成顶点的uv坐标，去贴图中采样。使用视差贴图和深度便宜营造出弹孔、抓痕的效果。

### 19、引擎的架构分层

1、工具层，一些列的编辑器，动作、材质、场景

2、功能层，让世界看得见(渲染)，动起来(动画，物理)，玩起来(输入)

3、资源层，处理数据、模型、音频

4、核心层，工具箱，内存管理，容器分配，数学运算，线程池

5、平台层，解决平台通用性，PC，Mac等

第三方插件





# 三、重点

## 1、GPU渲染管线

主要分为四个阶段

- **应用程序阶段**
- **几何阶段**
- **光栅化阶段**
- **像素处理阶段**

(1)应用程序阶段包含碰撞动画，游戏逻辑，层次视锥裁剪等优化算法，提交图元到GPU

(2)几何阶段，将三维空间的数据转化为二维空间的数据，分为顶点着色，mvp变换，视锥裁剪，透视除法获得ndc坐标，视口变换得到屏幕坐标。

(3)光栅化阶段，将图元离散化成三角形的过程，依次遍历每个像素，判断是否在三角形内部，然后利用插值得到该像素的属性(世界坐标，法线，uv)，属性值需要透视矫正。MSAA

(4)像素处理阶段，利用上面计算的信息计算每个像素点的颜色。

(5)测试合并阶段

**裁剪测试**，允许程序员开设一个裁剪框，只有在裁剪框内的片元才会被显示出，框外的都被剔除。当视口和屏幕窗口不一样大，就需要使用裁剪测试，避免一些浪费的渲染

**透明度测试**，某个片元的透明度小于等于某个阈值，就会被舍弃，比如在渲染草的时候，背景色的a通道较小，通过透明度测试舍弃

**模板测试**，屏幕上的每个像素点保存一个无符号的8位整数值，每次渲染比较片段的参考值和模板值，达到预期就通过，否则丢弃。

比如在渲染物体时开启模板测试，写入一遍模板信息，然后变大物体，关闭模板写入，设置不为1的地方渲染一个颜色，这样能在物体周围实现一个边框的效果 。 ***这里的fs次数可能比vs次数少***

**深度测试**，光栅化之后，zbuffer中存储每个像素的最小深度信息，opengl中01之间，且存储非线性深度，计算物体深度只有在小于zbuffer中的深度值时才会记录下当前深度并着色。

**缺点：**z冲突，两个非常近的物体渲染时，难以确定谁在前面

解决办法：移动其中一个物体的位置。或者 选择适当的远近裁剪平面，近平面尽量远，因为近平面处的精度非常高，将近平面往后调且保证物体不会被裁剪掉，就能避免z冲突。

**Early-z**技术，将深度测试提前，解决了之前深度测试在fs之后，计算完颜色，但是深度测试通不过就白计算的处境。

只会将通过深度测试的片段传到fs中

缺点：透明度测试，通过深度测试但是没通过透明度测试，会产生透明物体的背后不透明物体消失。

修改深度值会使其失效





## 2、阴影实现

#### 2.1、光照贴图Lightmap

**原理**：从光源方向去离线渲染一个物体，把结果存在一张贴图里，因为离线渲染的时候，如果光线和物体间有东西被遮挡，那么物体上该点就会存在阴影，对应lightmap上就会有一个阴影的值，渲染时候直接从贴图上采样即可。

缺点：没办法做动态阴影，只能存diffuse

#### 2.2、Shadowmap

**原理**: 渲染两次，先从光源位置渲染一边场景，获得相应的深度贴图，叫做shadowmap，然后从相机的视角去渲染场景，对于每个片段，找到其对应世界坐标，算出其在光源空间下的坐标，利用xy分量去SM中采样获得深度1，利用z分量获得深度2，比较深度，判断该点是否在阴影中。

**缺点**：

**阴影失真问题**，在物体上存在黑白条纹，产生的原因是sm的分辨率不足，一个像素的信息对应空间的一篇区域，当光源从侧面看去，会导致该区域一部分距离近，一部分远，中间是刚好相等，从而跟采样结果比较会有一部分在阴影中，从而产生黑白条纹。

解决办法: 增加一个偏移量，来比较深度。 

原理是黑白条纹的原因是部分区域的 实际距离比采样距离大，那么将实际距离减去一个偏移量，使得不产生阴影。但是会产生“彼得潘”的现场。对此常用的技巧是正面剔除，将所有的深度采样时采样到物体背面的深度，这样在物体表面的就不会产生失真。对地板和表面开口的物体无效。

**锯齿问题**

使用PCF技术解决，从深度贴图中多次采样，比如3*3的区域，将采样结果(01)进行加权平均，获得柔和的阴影效果。



问题：对于透视投影的贴图，如何计算？  点光源做阴影！

使用cubemap，从光源的地方向周围6个面做一个渲染，生成6个面的深度图，在使用时，将该点的像素位置跟光源方向做一个差，获得方向，从cubemap中采样，然后将值从01变到0到zfar，将之前的方向向量求得长度，比较两者的大小即可。



#### 2.3、PCSS

用于改进PCF的采样区域，考虑到对于靠近遮挡物的区域，需要较小的kernel，远离遮挡物的部分需要较大的kernel来达到足够的模糊效果。利用算法来估计出当前位置的采样区域，然后利用这个采样区域去计算PCf。

原理是：利用相似三角形，将光源的面积，阴影的面积，遮挡物到光源的距离，采样点到光源距离作出一个等式，只需要计算出光源到遮挡物的平均距离，即可计算出阴影的面积； 然后利用在一块区域，作为filtersize，做一次PCF。 

如何计算光源到遮挡物的平均距离，在着色点周围确定一个小区域，采样一些点，通过比较这些点在shadowmap中的深度和该着色点的实际深度，，确定哪些采样点在阴影中，将这些距离求个平均，即可得到平均遮挡深度。

可以通过泊松采样，或者逐点求。 前者有噪声，后者速度慢。

#### 2.4、VSSM

主要用于解决PCSS第一步和第三步太慢的方法。

- 解决第三步，已知一个区域，去计算PCF，也就是类似于给出一个分数，计算这么多人里面我排第几的问题，可以通过计算这个区域的均值和方差，问题转换为如何计算 任意区域内的均值， 通过SAT，快速求出某一个区域的均值，类似于二维的前缀和思想，方差通过经典的 平方的期望减去期望的平方， 可以在计算shadowmap的时候在rgba的另一个通道存储距离的平方，然后计算SAT，这样给定任意的区域，就能快速计算出期望和方差。 

然后通过切比雪夫不等式，近似估计，可以算出这个区域的遮挡率。、

- 解决第一步，求某一个区域内，遮挡住该点的平均深度(深度是sm中存储的值)是多少？

​	也就是某一个区域内，小于给定值的平均值是多少，将N1表示该区域中小于定值的采样点数量，N2表示大于的数量，N总数量，就有N1 / N * zocc(遮挡) + N2 / N * zunocc = zavg; 

通过切比雪夫可以得到这块区域中遮挡的概率，即 N1 /N, N2 / N得知，然后将avg通过SAT获得，最后近似未遮挡区域的平均深度就是 该点的深度， 获得遮挡区域平均深度的近似值。

#### 2.5、CSM

解决的问题：在一个比较大的场景中，如果使用一个固定分辨率的贴图，会导致近处的锯齿比较严重，但是又不能无限增加shadowmap的分辨率，会增加显存带宽。 比较好的方法是针对视锥进行shadowmap，而不是针对整个场景。

步骤：

1、将视锥分割成几个子视锥，确定znear和zfar。 可以通过固定比例，或者某个等比数列确定比例关系。在近处使用精密的贴图，在远处使用粗略的贴图。

2、用对应的子视锥以及光源方向确定相应子视锥的AABB包围盒(shadowmap的投影都是使用正交投影)，先确定最小包围球，在相机方向，可以得到子视锥在ndc坐标下的8个顶点，乘以投影的逆矩阵转到view space下，利用两个平面的对角线长度，以及zear和zfar的长度，可以计算出最小外接球，进一步得到AABB包围盒，同时需要保证光源能够看到该视锥中的所有物体，确定好每个子视锥下光源的lookat矩阵和投影矩阵。

3、渲染场景，将像素的每个z，确定相应在哪个视锥，然后利用相应的shadowmap和光源的vp矩阵，判断该点是否在阴影中。

问题：

闪烁、阴影正交相机一动，贴图中的每个像素的深度信息发生改变，但是每个像素改变程度不同，阴影形状就会改变。

解决：将贴图中每个像素对应场景大小的比例unit计算出来，将阴影相机坐标的xy值转换成unit，保证每个这个坐标换算后的小数位相同即可。

阴影存在缝的问题，就是多个shadowmap中切换不平滑，我们可以将子视锥的包围球变大，包围住两级cascade的过渡段，然后在这之间使用插值。



## 3、延迟渲染

### 3.1、延迟渲染

1、在多光源的环境下，渲染一个场景，比如n个物体，m盏灯，此时时间复杂度是O(n * m)，且很多三角形是看不见，光照也招不到，造成了巨大的渲染浪费。 我们可以将所有可见的像素点信息(位置、法线、颜色、镜面值)存储在缓冲中，这些都是经过深度测试最后跟我们在屏幕上看到的一样。管这些叫G-buffer，在后续渲染的时候，需要将这些信息渲染在一个跟屏幕一样大小的矩形上，每个像素只需要执行一次光照计算，即可达到一样的效果，时间复杂度变成O(n + m)

2、需要存储3张图，位置，法线， 颜色+镜面(RGB + A)， 对于PBR还要存一个金属度+粗糙度

### 3.2、延迟渲染和正向渲染的区别

**正向渲染，**先计算着色，后进行深度测试，渲染n个物体在m个光源下的着色，时间复杂度是n*m,且一个像素会进行多次fs，优点是带宽要求小。

**延迟渲染**，先进行深度测试，将所有看得见的像素所需要的信息存储在gbuffer中，再进行着色计算，每个像素只计算一边fs，时间复杂度n + m

延迟渲染的**优点**：将光源的数目和场景中物体的数目在复杂度层次上完全分开，渲染n个物体在m个光源下的着色，复杂度是n + m，只渲染可见的像素，节省计算量

**缺点**：

1、内存开销大，读写Gbuffer的内存带宽是性能瓶颈

2、对透明物的渲染存在问题，不能进行blending，所有的数据都从一个片段中来，而混合需要结合多个片段的数据。

3、只能使用相同的光照算法   4、对MSAA的处理 支持不友好



使用了3个RT，位置，法线(16位float)，颜色和高光(8位float，RABA)



实现点光源：

1、计算出每个光源的衰减半径r

2、正常渲染场景，获得zbuffer，和gbuffer

3、设置不可写深度，面剔除关闭，开始模板缓冲，并且设置背面深度测试不通过，模板缓冲+1， 正面测试不通过，模板缓冲-1

4、渲染一次所有的光球。  此时图元在球中的部分模板缓冲是正数

5、渲染场景到长方形中，开启模板测试，不为0就渲染，同时开始正面剔除，只渲染背面





**forward+**

可以解决许多灯光，但是每个灯的范围不是很大的渲染。

将屏幕做个细分，使用16*16个像素划分一个格子，计算这个格子被多少等照到，渲染这个格子，只需要迭代计算到的灯光，而不是所有的灯光。 可以通过将屏幕划分，通过computer shader来计算每个格子覆盖了多少光源，进一步提升效率。 光源剔除方法

 方法一、合并方法。在每一个计算着色器中构造光源索引列表，对于每个tile，基于最小深度和最大深度，构造一个基于屏幕的视锥，使用warp读取每个光源的信息， 如果光源对当前tile有影响，就利用原子操作将光源索引添加到线程的共享内存中。 多所有线程运行后，获得这个tile的光源索引列表，

方法二、分离方法，当光源数量比较多时， 对于每个光源使用一个线程，计算可以影响到的tile，然后将光源索引和受影响的tile写入缓冲，所有光源计算结束，对缓冲的数据按照tile索引进行基数排序，查找并记录每个tile的光源索引列表在缓冲中的开始和结束位置。

优点：前向渲染，支持透明物体，带宽利用率高， 使用z-prepass预计算算深度，提高效率

缺点：没有考虑深度信息，有些光源可能重复计算。  改进，使用cluster based， 由于透视投影，远处的块大一点，近处的小一点。





## 4、纹理贴图

### 4.1、立方体贴图

天空盒技术，营造出一个巨大的场景，提升玩家在场景中的真实感，比如远远的山、海、或者天空。  本质是6个面的纹理贴图，采样通过本次

判断在哪个面，然后通过uv采样。

两种渲染方式

1、先渲染天空盒，禁止深度写入， 然后开启深度写入，正常渲染物体，保证后面的所有物体都在天空盒的前面。

2、先渲染所有物体，后通过early-z技术渲染天空盒，将天空盒的深度设置为1，深度测试为小于等于，即只要没有物体，就渲染天空盒。

如果大部分遮挡，就选择第二种，效率更高。

此外，在渲染天空盒的时候，需要将viewmat的平移向量去掉，则位于1*1  * 1的天空盒始终位于原点，相机也位于原点，就能产生包围的效果。

(注意！！！！  viewmat并不是移动相机，本质上是相机在原点，去移动物体，所以即使lookat的position在立方体包围盒外面，只要移除了平移向量，立方体不会移动，也不会影响最后的效果。)

### 4.2、纹理贴图

将一张二维的图，按照一定的映射关系，将每个像素贴到物体表面的对应位置

纹理的坐标为uv，范围都在01之间，使用的时候直接去贴图上用 u*width, v *height去采样

(1)**环绕方式**，设置纹理坐标采样超出范围时，采取什么行为(重复，镜像，插值到边缘)

(2)**过滤方式**，当物体分辨率很高，贴图分辨率很低时， 设置采样方式，包括邻近采样，线性插值。

(3)**MipMap**

当物体的分辨率小于贴图的分辨率时，假设一个物体离屏幕很远，此时只占据了几个像素，但是却要从完整的贴图上采样，即一个像素对应贴图上多个纹理像素，不好采样，引入mipmap。将纹理划分为不同大小分辨率的图集，每次压缩到1/2，将邻近的四个像素求平均，多使用的内存量

1 + 1/4 + 1/16 +... = 4/3， 即多存储1/3

**如何确定在哪一层？**  对于屏幕某个像素，求其上和右的像素点，映射到纹理空间，求出相应正方形的近似边长L，此时层级等于log2L, 例如L = 1，D=0，即原始层，L = 2，D= 1，压缩的第一层。计算出的D并不是一个整数，可以采用四舍五入，或者三线性插值，去邻近的两个level里采样，将结果做个插值。

新问题：产生overblur，原因是mipmap默认压缩都是正方形压缩，对于长方形，或不规则图形，采样出错。 

使用各向异性过滤，解决对于长方形的条纹。

**更加快速**

对于远处的多个像素，对应同一个采样点，可以从Cache获得，效率比较快。



### 4.3、法线贴图

可以使低面数的网格产生更加精细、逼真的效果。给每个像素一个法线。

方法、通过切线空间存储法线，每个法线都指向正z方，因此法线贴图看上去偏蓝，取出法线后，转到-1到1，然后转换到世界空间

原因：解决存储在世界坐标中，在模型变换后，法线没有变换，或者法线每次都要随着模型的变换而变化，很麻烦。而存在切线空间，只需要根据三角形的三个顶点和uv坐标计算出TBN矩阵，然后将取出的纹理用TBN矩阵转到世界坐标下计算即可。

更好的方法，例如在phong着色模型中，将光线，视点，着色点转到切线空间，后续在fs中直接在切线空间里计算光照即可， 将运算量从fs中转移到vs中是可行的，因为绝大多数情况fs调用次数比vs多得多。

### 4.4、凹凸贴图

运用纹理来改变一个点的相对高度，从而改变法线，进而影响着色结果，将原本存储在法线贴图中的法线变成高度值即可。

![image-20220621232823234](image-20220621232823234.png)

### 4.5、置换贴图

真正改变了某一个点的高度，这样会使得物体表面产生各种阴影效果，更加逼真。



## 5、抗锯齿

锯齿产生的原因：采样的速度跟不上信号变化的速度，导致走样。

### 5.1、SSAA

提高每个像素点的采样次数，比如4xSSAA，原本屏幕的分辨率是800*600，将结果渲染到一个1600 * 1200的buffer上，然后下采样到800*600,在数学上是最完美的抗锯齿，但是光栅化和计算着色的成本都增加了4倍。

### 5.2、MSAA

在光栅化的时候，跟SSAA一样，一个像素内计算多个样本，同时计算一个覆盖率，在片段着色阶段每个像素只计算一次颜色值，以像素中央来计算，最后结果乘上一个覆盖率。 并没有将每个采样点计算一次颜色，而是每个像素计算一次，最后乘以覆盖率。

缺点：对延迟渲染支持不好。MSAA本质发生在光栅化阶段，也就是几何阶段后，着色阶段前，需要用到场景中的几何信息，但延迟渲染因为需要节省光照计算，将所有的信息存储在Gbuffer中，着色计算的时候已经丢失的几何信息，如何强行这么做，MSAA会增加数倍带宽消耗。

### 5.3、FXAA

是在图像的后处理阶段，一般需要在LDR下，SRGB空间中，而不是线性空间，抗锯齿效果是基于视觉的而不是基于物理的。

步骤：

1、将图像的边缘提取出来，通过给RGB每个通道一个权重，(G最多，R其次，B最少)

2、判断该点是否是边缘，通过采样该点和周围4个点，计算最大值和最小值的差值，判断是否超过一个阈值

3、判断边缘的走向，通过3x3的水平边缘提取算子和竖直的算子，计算出边缘是水平向的还是竖直向的，进一步判断是朝上下左右哪个方向(朝着差值最大的方向)。

4、往两边迭代，如果亮度值和该点的平均亮度差不超过一个阈值，则代表同一个边，否则就是找到了边界。

5、计算混合系数，越靠近端点混合系数就越高，并且近端点是同色端点时，不需要抗锯齿。

### 5.4、TAA

通过结合历史帧的方法，计算出当前帧和历史帧，然后通过一定的权重混合。

1、采样的时候根据halton2-3序列，修改projection矩阵，每次就能在相应偏移量采样。

2、对应静态场景，使用重投影的方式，记录上一帧的vp矩阵，先将当前帧进行vp逆变换到世界坐标，然后使用上一帧的vp矩阵变换到上一帧的位置，将两帧的结果按比例混合，一般来说是保存之前的许多帧，但是存储太大，只需要用上一帧的结果，然后用一个比例(当前帧占0.05)混合即可。

3、对应动态场景，物体在动，相机也在动，重投影就很不方便，可以使用Motion Vector，记录每个像素的偏移量，可以在延迟渲染的几何pass阶段，使用额外的贴图存储，通过记录上一帧的位置信息和投影矩阵，计算出上一帧的位置，然后利用当前帧和上一帧做差，写入到Motion Vector中，使用的时候当前帧先减去抖动偏移量，然后减去Motion Vector中采样的值，就能得到上一帧的位置，按比例混合即可。

问题：静态有闪烁，动态有鬼影。

主要原因是前后两帧的像素值差别很大，有可能是抖动，或者场景变换，光源变换等。比如遮挡关系，当前帧的像素点在上一帧中看不到。

解决办法：将采样的上一帧结果与当前帧比较，如果差别很大，就截断。 通过采样当前帧周围的9个点，得到rgb的最大和最小包围盒，然后可以直接clamp，或者使用clip，将历史帧数据进行修正。

### 5.5、DLSS

基于深度学习的超采样，直接把游戏结果实现四倍超采样。

## 6、PBR

### 6.1、**辐射度量学**

power, 辐射通量，表示单位时间穿过一个截面的光能

irradiance， 单位E，表示单位面积上的power

radiance， 单位L，表示单位面积，单位立体角上的power

**radiance和iradiance的区别**：radiance是带有方向的光，irradiance不带方向，各个方向上的radiance的积分和等于irradiance。

![image-20220627224655738](image-20220627224655738.png)

为啥有个cosθ,因为radiance是以垂直照射的面积计算，而irradiance是以实际接受的面积，所以每个光线都要乘以和法线夹角的cos

### 6.2、**BRDF** 

定义：双向反射分布函数，求出给定一条入射光，着色点，观察方向、(平面粗糙度)，返回从观察方向看上去的反射光占入射光的比例。

就是一个radiance / irradiance。  本质是一个零点几的小数。

问题：**为什么是radiance 比上irradiance， 而不是比上radiance**？

(1)从测量角度分析，出射光的radiance很好测量，但是入射光的radiance很难，因为需要保证光源大小跟单位立体角一样大。 因此，只需要保证光源足够小，且没有其他光源的干扰，就能获得来自光源方向的irradiance。

(2)从数学角度分析，照射进来一束光radiance，被接受后变成irradiance，然后散射到各个角度，只从其中一个角度接受，比值肯定是0，但是使用来自该方向的irradiance的微分，分母多了一个dw,比值小很多，不是0，有意义。

### 6.3、渲染方程

![image-20220627230833430](image-20220627230833430.png)

含义：从某个视点看特定的位置x，其亮度等于自发光项，加上四面八方入射光照射到这一点，然后反射到该视角的亮度之和。

### 6.4、PBR

#### 6.4.1 理论部分

基于物理的渲染是对基于物理原理的现实世界的一种近似。

包括三个条件：

1、基于微平面的表面模型

在微观尺度上任何平面都可以用称为微平面的细小镜面表示。 用粗糙度来表示这个微平面的排列。 一般来说越粗糙，排列越混乱，反射越发散。越光滑，光线更趋向一个方向，造成锐利的反射。

2、能量守恒

对于非发光物体，出射光的能量不会超过入射光的能力。光线到达物体表面，会被分离成折射和反射部分，前者进入物体表面被吸收，形成漫反射，后者是镜面反射部分。 但是对于金属而言，它的折射部分只会被吸收，而不会被散射，也就是金属不存在漫反射。 **实现的时候加一个金属度，将物体表面颜色和非金属的基础反射率F0根据金属度混合，即金属度为1的时候没有漫反射，反射的是物体的表面颜色**。 这就是金属工作流的实现。

3、应用基于物理的BRDF

对于brdf的常用模型

![image-20220627231658187](image-20220627231658187.png)

常用cook-torrance模型

![image-20220625220432757](image-20220625220432757.png)

其中D是**法线分布函数**

给定半程向量，判断微平面中多少法线和它同一朝向。 用粗糙度衡量，表面越光滑，反射程度越大

使用GGXTR

![image-20220625221333234](image-20220625221333234.png)

F是**菲涅尔项**，表示物体的反射率。 给定F0基础反射率，入射角和半程向量，利用fresnel-schlick近似求出反射率。

越靠近水平面，反射程度越大。  

![image-20220625221741182](image-20220625221741182.png)

对于金属而言，其基础反射率F0比较大(0.5-1.0)，因此用上面求解不对，对金属而言，没有漫反射，因此用表面颜色作为反射率，并用金属度作为参数，线性混合它和非金属的基础反射率，将结果作为F0。

G是**几何部分**，表示微平面之间的相互这档比例，在入射角平行平面的时候，遮挡比较多，可以用于修正从一个视角看球的边界(F反射比较多)光亮的情况。 使用史密斯法，将观察方向和光线方向都考虑进去。 传入一个观察方向，光线方向，法线，和粗糙度。返回一个未被遮挡的表面比例。

![image-20220625222944832](image-20220625222944832.png)

#### 6.4.2 计算实现

![image-20220627232620225](image-20220627232620225.png)

 对于每个直接光源，都需要计算漫反射部分和镜面反射部分，前者用kd* c / PI 计算，后者计算brdf项，两者相加乘以光源的radiance和夹角cos。

### 6.5、IBL

其光源不是直接光源，而是将周围环境整体视为一个大光源，使用立方体贴图每个像素采样点视为光源。 这种方式可以有效地捕捉环境的全局光照，使得物体更好地融入这个环境。

解决两点问题：给定任意方向wi，获取该方向的radiance； 解决积分快速且实时

我们需要将漫反射部分和镜面反射部分拆开计算

#### 6.5.1、漫反射辐照度

![image-20220627233412751](image-20220627233412751.png)

可以通过预计算，将场景中每个像素接受的各个方向的radiance存储到一张irradiance贴图上，采样方法是对基于该点法线的半球内的，所有光线进行黎曼积分求和，在实施渲染时通过法线采样即可作为该点的irradiance。

同时，需要更改kd部分，因为kd = (1 - ks)(1 - metalness) = (1 - F(h, v))(1 - metalness)， 其中h和wi，wo相关，不能直接提出，使用近似法线、w0， 粗糙度。

![image-20220627234142872](image-20220627234142872.png)

同时加入roughness来考虑到球体边界部分反射率较高，用来修正

#### 6.5.2、镜面反射部分

使用分割近似求和的方式，分割为基于光源的prefilterMap（预滤波环境贴图）和预计算的BRDF

其中，前者需要根据不同的粗糙度，将卷积结果存储在不同的MipMap中，处于中间的粗糙度可以插值。渲染时通过MipMap级数直接采样相应的贴图。跟irradianceMap的区别

(1)、求积分时，需要采样计算反射光线R周围的镜面波瓣的光线，同时考虑粗糙度，粗糙度越大，波瓣越大

(2)、结果采样时，通过反射光线采样，通过V和N求出反射光线

后者BRDF项，则是通过将菲涅尔项中的基础反射率提出等式，化成一个有关F0的函数，a*F0 + b,通过预计算可求出a和b的值，代表比例和偏差，然后存到一张2d查找贴图中LUT(look up texture)，存在颜色的R和G通道， 渲染时通过cos和roughness采样结果，并给出相应的F0，计算得到brdf项。



## 7、光照模型

### 1、局部光照模型

包括 Lambert，Gourand，Phong，Blinn-phong， Cook-toorrance模型

**问题1：Phong和Blinn-phong的区别**

计算高光时，前者使用反光光线和观察角度的余弦，后者使用半程向量和法线的余弦，后者的计算量更小，同时，后者解决了，在反光度较低的物体上，当反射光线和观察方向大于90度将什么都看不见的问题，使用半程向量会将角度控制在90度以内。

### 2、全局光照模型

包括Whitted光线追踪，PathTracing，双向路径追踪，

**问题2：光线追踪和路径追踪思想**

光线追踪：从视点向屏幕的每个像素发射光线，找到该光线和最近物体的交点，如果该交点是散射面，直接计算光源照射到该点产生的颜色，如果是镜面或者折射面，递归计算反射光线或者折射光线跟物体的交点，直到达到最大递归层数或者逃逸出场景。

路径追踪：在光追中没有考虑漫反射物体的随机反射，而是直接计算了着色，但实际上漫反射物体还是会朝各个方向反射光线，产生间接光照，因此提出路径追踪。

概念：从视点发射出一根光线，光线和物体表面相交时，根据物体表面属性继续采样一个方向(随机方向)，发出另一条光线，递归采样，直到打到光源上或者逃逸出场景，利用蒙特卡洛积分的方法，计算其贡献值，作为该像素的颜色。

对于一个像素点，打出若干光线，最后计算平均值。   使用俄罗斯轮盘赌结束采样，光线以一定概率p发射，如果和光源相交，表示直接光照，直接利用蒙塔卡洛积分计算结果，最后除以p，如果和物体相交，递归计算反射光线的着色结果，并将着色结果当成一个光源的亮度，用来计算积分，最后都除以p。 达到无偏性。

减少噪声，对于直接光照，我们考虑从光源处采样，利用公式dw = dA / r * r， 将渲染方程变成跟dA相关的，最后在光源处采样，在光源积分。



#### Sphere Harminic(光照探针)

目的：使用不同阶的球谐函数和系数就能表示出环境光，本质是一系列二维的基函数，将反射方程中两个函数在半球上的积分，转变成了相应系数的点乘，只需要预计算的时候求出系数，在渲染的时候利用系数就能还原出shading和shadow

具体步骤：分为light和light transport部分，对于light，只需要在求f(x)B(i)在半球上的积分，对每个基函数i，求出对应的系数即可。 在使用的时候采用黎曼积分，对于立方体贴图的每个像素，换成方向dir，然后利用dir和l、m，求计算一遍对每个sh的系数贡献，遍历完所有的像素点，就能求出light 的球谐系数。

反射方程中，将light部分分为 基函数和系数，提取出系数，将后面的函数再做一次投影，light transport项，对于场景的每个顶点，我们需要计算其shadow部分，brdf * V*dot(wi,N) ,  采用蒙特卡洛积分的方式，在每个shading point的球面上采样，计算出每个sh的系数。

在实时渲染中，直接在vs中，将对应的两部分的系数做个点乘，就能得到该顶点在环境中的着色情况，然后传到fs中，对每个像素做个插值即可。

**和IBL的区别：**只支持低频的环境光，而IBL支持高频的



LightMap，基于预计算的光照贴图，

LightProbe，在空间中撒上许多采样点，计算每个采样点的光场，然后当人在空间中移动的时候，找到周边的probe，然后插值计算出自己的光照

#### RSM

Reflective shadow maps

先从光源处做一次shadow map，记录能看到的像素块，然后正常渲染场景，对于每个着色点，通过反射方程计算该点的间接光照。 适合做一些手电筒场景，覆盖的场景小，对RSM的分辨率比较低，效率就更高。

几个要点： 

1、在p上求积分，变成在反射物q上求积分，通过 dw = dA / r^2 改变反射方程，

2、q点的radiance 等于 brdf * irradiance， = brdf * φ / DA， 最终可以约掉dA

#### LPV

Light Propagation Volumes 光线传播体积

本质：对于任何一个shading point，获得间接光照到达这个点的radiance

步骤

1、将场景划分为网格，用来传播radiance(先计算好光照再传播)，通过RSM

2、注入，将间接光源放在格子中，将格子内部的所有间接光源都加起来，获得一个朝四面八方的radiance初始值，使用SH压缩

3、传输，通过多次迭代，所有格子每次都向空间中的6个方向传播一次，求和(假设反射物都是diffuse的)

4、正常渲染，每个像素找到对应格子里的间接光照

问题：漏光，有一堵墙，左边是直接光，右边就会有间接光，这是不正确的。 只能通过改变格子大小减缓这种情况。

#### VXGI

1、将场景分成一堆离散的三维格子，组织成一颗树形结构。

2、先做一遍直接光照，记录每个格子里间接光照的入射方向和法线

3、从相机出发，发出光线到达物体上，反射光线打出一个锥，判断锥和场景中格子存储的间接光照，最后计算。(支持glossy)





#### SSAO

首先通过深度测试生成一张深度贴图，对渲染的每个像素，在其切线空间确定的半圆空间内随机采样，将这些采样点的pos变换到屏幕空间，进行深度对比，确定一个遮挡比例，生成AO贴图。参与最后的光照计算。

缺点：由于视点移动和随机采样，会有噪点，使用平滑过滤消除。



#### SSDO

步骤：

1、从camera视角生成深度图

2、在着色点半球内随机采样，判断采样点的可见性

3、如果不可见，就计算该点作为间接光源，根据反射方程计算其对着色点的贡献



### 3、采样方式

**蒙特卡洛积分**：解决定积分的问题，对于难求原函数的情况下，利用数值的方式近似计算定积分。计算方式是以某一pdf获取一个采样点，计算出结果并除以pdf，最后加权平均，只要有足够多的样本，就能达到数值的近似解。

有些估计是无偏的，随着样本数量的增加，最终会收敛到精确解

但是有些是有偏的，生成的样本并不是完全随机，趋向于特定的值或者方向。具有更快的收敛速度，但有可能永远也不会收敛到精确值。



问题：如果使用蒙特卡洛积分求解fx = x 在01上的积分值？？

核心思想：在规定的区域内，比如边长为1的正方形内，随机撒一些点，然后判断这些点在不在规定区域内，通过数量除以总数，就是概率， 然后通过正方形面积乘以概率，就是要求图形的面积。  

```c++
std::random_device rd;
std::mt19937 gen(rd());
std::uniform_real_distribution<float> rng(0.0, 1.0);
//如果是0到2之间呢？ 也是这样求，求出概率后 * 4， 等于边长为2的正方形面积

int main() {
    int n;
    cin >> n;
    int inside = 0;
    float x, y;
    for (int i = 0; i < n; i++) {
        x = rng(gen);
        y = rng(gen);
        if (x > y)
            inside++;  //判断点在y = x 下面
    }
    printf("%f\n", 1.0 * inside / n);
}

```

如果重要性采样呢？ 加快蒙特卡洛积分收敛速度？





**重要性采样：**通过现有的分布函数，想办法在被积函数分布可能性较高的区域进行采样，进而达到高效预估结果的一种策略。

可以基于低差异化序列，vander Corput， 将整数转成2进制，然后翻转到小数部分，比如2就是10--0.01， 4就是100 == 0.001。 然后基于粗糙度和低差异序列，获得一个围绕微表面半程向量的采样向量，反推出光线方向，采样结果并加权平均，这里假设观察方向等于镜面反射向量等于立方体贴图传入的方向向量。

比如路径追踪中，随机生成一条光线，如果最后都没和光源相交，这样计算就浪费了，我们尽可能去采样那些能打到光源上的光线，达到最终更优的结果。

**如何实现？？？？**





## 8、空间加速结构

介绍一下几种空间加速结构

- 四叉树，八叉树。将场景平均分成4、8块，每个几点表示一个面积、体积，使用是先判断和父节点能否相交，如果能继续和子节点判断。 
- BSP，使用一个特定的多边形进行分割，将空间分成两部分，特点是：平衡的，每个叶子节点的深度接近，可以获得严格的从后往前的顺序，通过画家算法来绘制整个场景。 KDTree，按照xyz的顺序切割轴。不适合动态场景，且构造时间偏长。
- BVH，按照物体进行划分，每次按照物体重心排序，可以依次按照xyz的顺序，左右各划分一半的物体，使得每一块区域内的物体数量尽可能相等。 SAH，解决了当空间物体分布不均匀时，使用BVH造成包围盒很多重叠，对于n个物体，有n-1种情况，每次遍历所有情况，选取一个代价最小的。 每种分配的代价等于 左区域的表面积 / 总面积 * 左区域的数量 + 右区域的表面积 / 总面积 * 右区域的数量。

1、八叉树空间划分，场景中的物体会动怎么办？

答：八叉树是四叉树在三维空间的变种，将场景划分为8个子空间，可以用于加速视椎体裁剪，加速射线检测，碰撞检测，加速范围内的物体检测。

场景中物体运动的时候，先把物体从节点中删除，并插入到新的节点中，为了加速，可以先检测该节点是否还位于父节点中，减少开销。



## 9、骨骼动画

在游戏动画中，一个Pose包含一个骨架和多个局部关节姿势以及全局关节姿势；

​		其中骨架包括若干关节，从Pelvis开始，到四肢去，每个关节都记录一个其父节点的索引，同时存一个绑定姿势的逆变换矩阵(从模型空间变换到该局部空间的变换)

​		局部姿势则是从该节点局部空间变换到其父节点的矩阵变换，全局姿势则是累乘局部姿势到模型空间所得的矩阵。

​		蒙皮就是每个蒙皮网格的顶点会追随其绑定的关节而移动(一般为4个关节，最后通过一个加权平均来计算最后的位置)。 

蒙皮矩阵就是将**网格顶点从绑定姿势变换到当前姿势**； 关键：顶点位置相对于局部空间的坐标不会改变

![image-20221205105806626](image-20221205105806626.png)

其中，绑定姿势的逆矩阵在游戏中是常量，创建模型是就确定了。 当前姿势矩阵需要没帧更新，从每个节点的局部姿势累乘到根节点所得。



# 四、扩展

## 1、游戏中实现点选物体的功能

（1）、通过屏幕上的一点，求出其世界坐标(假设z = -1)，利用相机的坐标求出光线，利用光线求交

## 2、在一个三角形中均匀采样

(1)、通过最小包围正方形，产生一个点，然后判断这个点是否在三角形内

(2)、将三角形中心对称变成平行四边形，然后移动凸出右边一块到左边，变成矩形，然后随机采样，采样在左边的移动到右边，然后采样不在原三角形的中心对称到原三角形。

![image-20220711111357355](image-20220711111357355.png)

(2)、通过重心坐标



# 五、OpenGL知识点

## 1、不同的shader

(1)vertex shader

功能：将位置（世界坐标以及裁剪坐标），法线，纹理等信息发送到光栅化阶段，用来插值。

(2)Geometry shader

输入：一个图元，点或者三角形，可以将其变换为不同的图元，且能够生成比原来更多的顶点。

优点：对于一些简单且重复的形状来说，在几何着色器中处理比在顶点缓冲中手动处理高效的多。

几何着色器的输入是一个图元的一组顶点，通过EmitVertex提交顶点和EndPrimitive绘制图元

 比如输入一些点，根据这些点生成相同形状。

实现爆破物体的效果，传入三角形，将每个顶点沿着面法线方向移动。

法线可视化，先正常将渲染物体，然后渲染每个顶点的法线，通过顶点着色器传入位置和法线，在位置和法线方向画线。

(3)fragment shader

通过传入的每个片元的位置，法线，纹理坐标等，计算光照和阴影。

(4) computer shader

功能：跟其他着色器不同，主要用于通用并行计算，独立的图形管线，没有用户定义的输入输出，可以看做GPU上运行的

将本地工作组的大小设置为64的倍数， 对于A卡，一个warp64线程，对于N卡则是32线程。能够适配。

否则小于32时，其余的线程就闲置出来了，效率较低。



## 2、GPU硬件架构

问题：

1、GPU是如何与CPU协调工作的？

答：对于分离式架构，即CPU和GPU都有各自的缓存和内存，CPU将主存的处理数据复制到显存中，然后通过GPU驱动程序驱动GPU，在其完成并行计算后，会将结构存储在显存上，然后GPU将结果传回CPU。

2、GPU也有缓存机制吗？有几层？它们的速度差异多少？

答：有，包括寄存器，共享内存，L1缓存，L2缓存，纹理、常量缓存，全局内存 共5层。 速度对于寄存器1个周期，共享内存和L1L2缓存1-64个周期，最慢的是纹理和全局内存400-600个周期。

**3、GPU的渲染流程有哪些阶段？它们的功能分别是什么？**

答：

**先介绍一下Fermi的GPU架构**，每个GPU里包含多个GPC(图像处理簇)，每个GPC包含多个SM(流多式处理器)，每个SM中包含，加载存储单元，若干SFU。 L1\L2等缓存。32个core，运算单元，还有32个为一组的线程组成的线程束warp，通过线程束来指挥数据在core上运行，这是典型的SIMT，一条指令多个线程同时执行，此外再每个core内还有8个ALU(逻辑运算单元)，负责运算两个vec4的数据，是SIMD的构造，这里会有个优化，co-issue，会将数据不足vec4的类型拼凑成vec4，比如vec3和float。但是对于一个变量即使输入也是输出的时候不会被优化。     

对于warp有**锁步执行，**如果不能执行，碰到if else 或者break的情况，该线程就会被maskedout，等待其他线程执行完。 所以shader中要少用if else。

**克服时延**，对于某些指令需要更长的运行时间，比如内存加载，warp调度器就会切换到另一个没有内存等待的warp继续执行，可以是顶点、片段等着色器，目的就是为了提高ALU的吞吐量。

执行顺序：

1、程序通过图形API发送drawcall指令，指令被推送到驱动程序，检查指令的合法性，然后把指令放在GPU可以读取的push-buffer中，下一步驱动程序把push-buffer中的内容发送到不同GPU的不同SM中，GPU通过主机接口接受命令，通过前端处理命令。 在图元分配器中开始分配，将提交的n个三角形分配分给若干的GPC，并且将相应的索引下标进行简化，都从012开始。同时顶点数据也被vertex fetch运到SM中的L1cache中。

2、每个SM的vertex fetch模块根据索引从相应存储取出顶点数据。warp scheduler分配指令，每个线程根据指令指挥一个数据到相应的core内执行计算。

(1)、顶点着色，将顶点坐标变换到裁剪坐标，曲面细分，几何着色器，流输出等。将顶点进行装配，变成三角形，进行裁剪，一般更优的是对近平面裁剪，其他超出视椎体的不管，因为不会被光栅化。经过透视除法、视口变换，将坐标变换到屏幕坐标，

(2)、然后进行光栅化操作，先求出三角形的最小外接矩形，每个三角形的块根据它在屏幕的位置将它分配到不同的光栅引擎中， (不同的三角形在相同的区域上，会被分到同一个光栅引擎中，需要保证顺序(避免相同深度的图元出现随机遮挡情况)。 在逐像素之前进行一个z-cull，维护每个块的最大深度，如果另一个相同块的最小深度比这个值还大，就说明会被遮挡，不被光栅化)。  在光栅化时，就需要判断这个顶点是否在三角形内，可以通过重心坐标，如果是就将该像素和周围2x2的都传入fs中。  并且通过透视矫正插值计算该像素的所有属性，位置、法线，uv。 这里可以执行面剔除，以及MSAA

(3)、再使用warp执行像素着色器，计算出每个像素的颜色

(4)、根据三角形的相应顺序，转到ROP模块，跟光栅引擎一样，每个ROP管控屏幕的一个区域，计算相应的像素的最小深度，以及记录其颜色，模板测试，混合等。

**4、Early-Z技术是什么？发生在哪个阶段？这个阶段还会发生什么？会产生什么问题？如何解决？**

答：提前深度测试，将原本的深度测试提前到光栅化和片段着色器之前，使得通不过深度测试的片元不去计算相应的着色，减少运算量。

问题：失效情况(1) alpha test (2) 修改深度  (3) 使用Tex kill (4)其他需要混合后面颜色的操作

会导致深度数据冲突，比如5通过early-z，在写入zbuffer之前，另一个10也通过了z-buffer，这样就会导致后续的10将前面的5覆盖，出现错误的结果。   避免深度数据冲突，在写入深度之前，再次与frame buffer中的对比。

**5、SIMD和SIMT是什么？它们的好处是什么？co-issue呢？**

答：SIMD单指令多数据，使用一条指令处理一个vec4的向量运算，在游戏引擎中常用SSE指令集，采用128的寄存器，存储4个32位的float型浮点数。 SIMT是单指令多线程，在原先的基础上加入多线程， 对于fermi架构，一个SM里包含一个warp，32个线程，发送一条指令，32个线程同时执行，只是线程的数据不同。

co-issue是为了充分利用SIMD，比如可以将两个低维的运算合并成一个高维的，比如1D和3D。 但在对于一个向量既是操作数，又是存储数，无法使用。 

**6、GPU是并行处理的么？若是，硬件层是如何设计和实现的？**

答：是，有若干的GPC，每个CPU包含若干SM，在SM中有warp，每个warp包含32个线程，用来执行指令，并且包含32个core，每个core里包含8个ALU，一次可计算两个4维向量，比如顶点着色器功能传入32个顶点，分到相应的core里执行相同shader指令，进行相应的计算。

**7、GPC、TPC、SM是什么？Warp又是什么？它们和Core、Thread之间的关系如何？**

答：GPC图形处理簇，TPC纹理处理簇，SM流多处理器，warp线程束。一个SM包含多个core计算单元，32个线程称为一个warp，通过SM里的warp调度器调度这些线程去执行指令。

**8、顶点着色器（VS）和像素着色器（PS）可以是同一处理单元吗？为什么？**

答：是，统一架构着色器，为了最大化利用GPU的运算能力，可以使得一些core在执行vs,另一些可执行fs，不会有空闲。此外还可以减少GPU的硬件单元，压缩物理尺寸和耗电量。

**9、像素着色器（PS）的最小处理单位是1像素吗？为什么？会带来什么影响？**

答：不是，是2x2的像素块，为了精简SM架构，减少硬件单元和数量，并且可以用无效的像素块辅佐有效地像素块求导。

**10、Shader中的if、for等语句会降低渲染效率吗？为什么？**

答：SIMT下，单指令执行多线程是有代价的，因为很有可能存在相同的指令，但是数据不同无法进入相同的模块，比如if语句，和for循环的循环次数可变时，此时相应的线程就会遮掩，也就是要等待其它线程执行完，这样会把所有的分支都走一遍，降低了渲染效率。

**11、如下图，渲染相同面积的图形，三角形数量少（左）的还是数量多（右）的效率更快？为什么？**

![img](https://img2018.cnblogs.com/blog/1617944/201909/1617944-20190906000232145-1801159116.png)

答：面片少的更快，在vertex fetch阶段，传输的节点更少，带宽更少， 不同的三角形分配到不同的SM中，消耗性能也就越多。

**12、GPU Context是什么？有什么作用？**

答：为了解决warp在执行指令时，出现I/O，需要等待的情况，这时候就切换一个warp来执行，也就是在每个core内缓存多个warp执行不同的指令，可以是顶点、像素、片元shader等。 想法就是提高ALU的吞吐量。

**13、造成渲染瓶颈的问题很可能有哪些？该如何避免或优化它们？**

答：1、减少CPU和GPU数据交换，

​			(1) 减少draw call-  使用**合批Batch**

​			(2) 开启LOD，减少顶点数，三角形数

​         （3）**避免从显存读数据**

​    2、减少overdraw

​          (1) 避免Tex kill, Alpha test， Alpha blend

​        （2）剔除算法：层次视锥剔除，齐次裁剪、背面剔除，early-z 

​          (3)  减少粒子数量

  3、shader优化

​        (1) 避免if switch分支

​       (2) 避免for循环语句，使用break或者循环次数可变的计数

​        (3) 减少纹理采样，禁用discard， 减少复杂数学函数调用

​		(4)将尽量多的运算放置顶点着色器，比如TBN矩阵，不要将每个像素点的切线空间的法线转到世界空间，而可以在顶点着色器中，将视点、观察位置，光的位置转到切线空间，加快效率。



**SIMD的指令运算，基于SSE寄存器**

答：SSE寄存器是128位float类型的寄存器，可以存储4个float数，使用的时候需要将数据通过__mm____load__ps将float的数组存储到____m128类型中，调用相应的指令，比如mm add ps, mm mul ps等计算加减乘除，编译器会自动把数据放到sse寄存器中，最后将计算结果通过mm store ps返回到float数组中， 注意float数据要保持16字节对齐。

使用矩阵乘法，将某一行vx,vy,vz, vw, 通过mm shuffle ps 将每个数据扩充，赋值成4个到sse寄存器中，然后将该__m128数据乘以矩阵的第一行，通过mm mul ps，再调用mm add ps将四行的结果累加起来，就能得到计算结果。



## 杂项

### 1、GLFW作用

创建和管理窗口和opengl上下文，同时提供处理手柄、键盘、鼠标输入的功能。

### 2、GL如何在shader中进行纹理采样？

GPU有16-32个纹理槽，每个纹理槽可对应多个纹理单元，在drawcall之前激活相应的纹理槽，然后绑定纹理单元，同时绑定相应着色器采样器到相应的纹理槽。  在片段着色器内，定义uniform的 sampler2D的变量，

后续就能在fs中通过texture采样



API 

```c++
glCreateShader  ----> glShaderSource  ---->  glCompileShader
    glCreateProgram --> glAttachShader ----> glLingkProgram
```



### 3、如果对一个缩放的模型，进行法线修改

```c++
Normal = mat3(transpose(inverse(model))) * aNormal;
```

### 4、如何快速判断是vs和fs哪个对效率影响大？

fs，降低屏幕分辨率，如果帧率明显上升，说是fs是瓶颈。

vs，通过调整顶点格式大小，比如每个顶点多传几个uv坐标，或者简单的改变顶点程序的长度。

纹理，将texture采样到mipmap最低层，如果帧率明显上升，则纹理是瓶颈。



### 5、VAO，VBO，IBO

VAO是顶点数组对象，在opengl的核心模式必须使用，使用前将VBO和 IBO绑定在VAO上，指定布局方式，每次在drawcall之前，只需要绑定相应的VAO就能绘制相应对象，无需每次重新绑定VBO并指定布局。

VBO是顶点缓冲区对象，用来存储大量的顶点信息，包括位置，法线和uv等，在调用drawcall后将传到GPU内存中。

IBO是索引缓冲区，往往在一个对象中有多个边共享一个顶点数据，使用IBO就能减少大量的内存消耗，只需要通过索引下标来指定图元，在绘制时调用glDrawElement。

![image-20220721230246828](image-20220721230246828.png)

### 6、混合 blending

公式 Result =  Cs * Fs + Cd * （1 - Fs）;  源目标颜色就是当前fs计算的结果，Cd是当前存储在颜色缓冲中的颜色， 按照比例混合。

glBlendFunc(Fs, Fd)

### 7、GLSL语言

运行在GPU上的程序，类似于C语言，在着色器开头声明版本， 不同的着色器之间通过in out关键字实现数据的传输，当类型和变量名都相同会自动链接在一起。 另外可以通过uniform变量从CPU向GPU发送数据，在着色器中声明uniform变量，在主程序中通过getuniformlocation获得地址，并通过uniformxx传输数据。

### 8、CPU和GPU之间的调度

opengl的主程序由CPU运行，图像处理部分通过GLSL交给GPU执行，

数据传输步骤

1、利用内置的opengl函数生成一个ID

2、根据ID进行内存类型的绑定。 此时GPU中用于接受系统内存数据的标识符就准备好了

3、对这部分的内存初始化，数据来自于glBufferData绑定的内存，数据提交到GPU之后，根据应用场景对数据进行分配。



### 9、帧缓冲

颜色缓冲，深度缓冲，模板缓冲等合起来成为帧缓冲。

一个完整的帧缓冲需要包含：

- 附加至少一个缓冲，颜色，深度或模板
- 至少有一个颜色附件，且有相应内存
- 每个缓冲都应该有相同的样本数

纹理附件：三种都可以

渲染缓冲对象：深度缓冲和模板缓冲



创建和使用

```c++
Gluint fbo;
glGenFramebuffers(1, &fbo);
//绑定
glBindFramebuffer(GL_FRAMEBUFFER, fbo);

//创建一个附件，包括纹理或渲染缓冲对象

//纹理附件，类似三个缓冲，可以直接读写
Gluint texture;
glGenTextures(1, &texture);

glBindTexture(GL_TEXTURE_2D, texture);
glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, 800, 600, 0, GL_RGB, GL_UNSIGNED_BYTE, NULL);
//设置为空的内存纹理
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);

//将颜色附件附加到帧缓冲上
glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, texture, 0);
//将深度和模板缓冲纹理附加到帧缓冲， 24位深度、8位模板
glTexImage2D(
  GL_TEXTURE_2D, 0, GL_DEPTH24_STENCIL8, 800, 600, 0, 
  GL_DEPTH_STENCIL, GL_UNSIGNED_INT_24_8, NULL
);

glFramebufferTexture2D(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT, GL_TEXTURE_2D, texture, 0);

//渲染缓冲对象，将所有的渲染数据存储在缓冲中，不左任何纹理格式的转换，写速度比纹理附件更快，但是不支持读取，可以使用glReadPixedls从帧缓冲中读
Gluint rbo;
glGenRenderbuffers(1, &rbo);
glBindRenderbuffer(GL_RENDERBUFFER, rbo);  //通常用于深度和模板缓冲
//创建深度和模板缓冲
glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH24_STENCIL8, 800, 600);
//附加到帧缓冲上
glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT, GL_RENDERBUFFER, rbo);

//最后使用之前取消 默认的帧缓冲
glBindFramebuffer(GL_FRAMEBUFFER, 0);
//然后使用自己的帧缓冲
glBindFramebuffer(GL_FRAMEBUFFER, framebuffer);

//最后渲染到 一个跟屏幕一样大小的四边形上，
```

### 10、高级数据和高级GLSL

**高级数据**

glBufferData来填充缓冲对象所管理的内存。 使用glBufferSubData可以指定填充缓冲的特定区域

```c++
glBufferSubData(GL_ARRAY_BUFFER, 24, sizeof(data), &data); //从偏移量24开始填充 sizeof(data)的大小
```

使用glMapBuffer获取当前绑定缓冲的内存地址

```c++
void *ptr = glMapBuffer(GL_ARRAY_BUFFER, GL_WRITER_ONLY);
memcpy(ptr, data, sizeof(data));// 使用场景，当直接从文件中读取数据，复制到缓冲内存，不需要先读到内存中
glUnmapBuffer(GL_ARRAY_BUFFER);  //解除映射
```



**分批顶点属性**

改变以往的顶点存储方式 ， 位置1，法线1，uv1， 位置2，法线2，uv2.。。

我们使用位置，法线，uv的存储方式，优点是从obj中读取的结果就是分离的，如果再重新交错，比较费时。

可以使用glBufferSubData来设置内存

```c++
float positions[] = { ... };
float normals[] = { ... };
float tex[] = { ... };
// 填充缓冲
glBufferSubData(GL_ARRAY_BUFFER, 0, sizeof(positions), &positions);
glBufferSubData(GL_ARRAY_BUFFER, sizeof(positions), sizeof(normals), &normals);
glBufferSubData(GL_ARRAY_BUFFER, sizeof(positions) + sizeof(normals), sizeof(tex), &tex);
```

同时改变布局方式,只需要改变布局和开始的偏移量

```c++
glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), 0);  
glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)(sizeof(positions)));  
glVertexAttribPointer(
  2, 2, GL_FLOAT, GL_FALSE, 2 * sizeof(float), (void*)(sizeof(positions) + sizeof(normals)));
```



**高级GLSL**

内置变量  gl_Position, gl_PointSize

fs中， gl_FragColor,  gl_FragCoord对应片段的屏幕坐标和深度值，gl_FrontFacing 表示该片段在正面还是背面

gl_FragDepth修改当前片段的深度值



**接口快**

能够将以前一个个的 in, out 传递变量，改成一次性传递

```c++
out VS_OUT  //当名字一样时，就会匹配起来
{
    vec2 TexCoords;
} vs_out;

void main()
{
    gl_Position = projection * view * model * vec4(aPos, 1.0);    
    vs_out.TexCoords = aTexCoords;
}  
```

**uniform缓冲对象**

解决一个问题，当有多个shader 时，我们需要设置多个相同的unifrom变量，比如shader1，shader2使用相同的view 和projection， 我们使用缓冲对象，来从同一块内存中读取。   将缓冲对象和shader都绑定到相同的绑定点即可



方式，在不同的shader内

```c++
layout(std140, binding = 0) uniform Matrices   //需要version 420  ，设置绑定点
{
	mat4 projection;
	mat4 view;
};
```

在渲染外，设置缓冲对象

```c++
GLuint uboMatrices;
glGenBuffers(1, &uboMatrices);
glBindBuffer(GL_UNIFORM_BUFFER, uboMatrices);
glBufferData(GL_UNIFORM_BUFFER, 2 * sizeof(glm::mat4), NULL, GL_STATIC_DRAW);
glBindBuffer(GL_UNIFORM_BUFFER, 0);   //设置一个缓冲对象，分配足够的内存

//缓冲对象绑定到绑定点0
glBindBufferRange(GL_UNIFORM_BUFFER, 0, uboMatrices, 0, 2 * sizeof(glm::mat4));
//开始绑定数据
glm::mat4 projection = glm::perspective(glm::radians(camera.Zoom), (float)SCR_WIDTH / (float)SCR_HEIGHT, 0.1f, 100.0f);
glBindBuffer(GL_UNIFORM_BUFFER, uboMatrices);
//分配数据
glBufferSubData(GL_UNIFORM_BUFFER, 0, sizeof(glm::mat4), glm::value_ptr(projection));
glBindBuffer(GL_UNIFORM_BUFFER, 0);
```



### 11、实例化

目的：使用一个渲染调用来绘制多个物体，节省每次绘制物体的前的CPU-GPU通信，减少消耗。

方法一、使用glDrawArraysInstanced ，指定绘制数量

同时使用gl_InstanceID即可指定当前实例的ID，在顶点着色器中设置uniform偏移数组，通过实例ID访问相应偏移量，绘制相应的数据。

方法二、当渲染数量非常多时，会超过uniform数据上限，

将偏移量存储在缓冲区中，设置布局方式，并且通过glVertexArrtibDivisor设置何时更新顶点属性offset，设置为(2, 1)即属性2在更新一次实例的时候，更新数据。 即每个实例使用的偏移量都不同。



### 12、Blooming

目的：让一个明亮的物体有一种真实发光的感觉，极大提升场景中的光照效果。

步骤：

1、提取场景中的高亮部分 

2、通过降采样，在最底层使用高斯模糊

3、上采样每次都和原来的图融合一下



详细步骤：通过将两个颜色缓冲的纹理附件绑定到帧缓冲上，渲染一边场景，多目标渲染，颜色缓冲1正常的场景，缓冲2存储高亮部分，通过给每个分量乘上一个比例， 再设置两个帧缓冲，将高斯模糊分为水平和竖直的，将上一步的高亮部分在两个缓冲中进行多次水平和竖直的模糊，最后将结果和原图进行叠加，最后tone mapping和 gamma矫正。

### 13、defered rendering

目的：解决多光源渲染问题

传统的正向渲染，对于m个物体，n个光源的时间复杂度是Omn,在光源很多的时候非常的耗时，并且很多片段并不会被光源照到，造成了不少的浪费。

延迟渲染包含两个阶段，几何pass和光照处理pass， 使用gbuffer存储位置，法线，颜色和镜面值，pbr还要存储粗糙度和金属度，先正常渲染一次物体，将所有能看到的像素点的信息都存储在Gbuffer上，然后在光照计算结算，渲染一个跟屏幕一样大小的矩形，从gbuffer中采样所需的信息，计算光照。  时间复杂度从mn 降到了m + n

缺点：

1、GPU显存消耗大，读写gbuffer的内存是性能消耗的瓶颈

2、不能使用blending，几何阶段保留的只有最近的看的见得像素信息，上一层的都丢失了

3、对MSAA支持不好，MSAA在光栅化阶段，需要图元的几何信息，但延迟渲染在几何阶段处理后已经丢失了几何信息，强行这么多的代价非常高。 一般使用TAA

4、只能使用同一套光照算法。

#  



# 项目遇到的问题

## 1、深度缓冲问题

在解决shadowmap的时候，考虑到生成的是平行光，所以光源使用正交投影，在物体处使用的是透视投影，所以需要将深度值转化为01之间，一开始用的深度信息就是viewspace下的信息，后来在考虑转到0-1之间，使用了如下几个方法

1、将ndc坐标下的z值转到01之间，直接使用，出问题了。 原因是投影之后，因为nf默认是负数，但是转换之后需要深度考虑为正数，所以最后的结果需要取个反

2、使用非线性变换，将viewspace下的深度，利用公式 (1/z - 1/n)  / (1 / f - 1 / n);

**reversed-z**

float类型的存储

1个符号位，8个指数，23个尾数。

其中指数部分用移码表示。 (移码部分就是 补码的符号位取反， 补码就是正数是本身，负数是反码 + 1)

```c++
例如 2的移码 1000 0010
    -2的补码 1111 1110  -2的移码 0111 1110
结论就是 算出指数x  使用x + 127 当做指数部分
```

尾数部分是23位，因此最小值时2^-23次， 转化为10进制 约等于 1.000000119.. (科学技术法，额外加上1)

其次，2^-22 约等于 1.00000023   所以我们无法表示1.0000001 到1.0000002小数，一般来说就是对应一个十进制数，从左到右第一个非0数算起，大约前7位数准确的。

比如0.1表示成二进制是一个无限循环小数，只能使用近似的方式表达。

结论：尽管float表示的范围很广，但是存在精度损失，对于数据只保证有效位7位是精准的，其余的均表示一个范围。 并且由于幂次的放大作用，导致离0越远 精度损失越大，离0越近，精度损失越小。



为什么要用reversed-z？  

结论：经过投影变换后，深度信息变成了非线性的，导致越靠近0的地方 有效数据越密集，精度越大，越远的地方，数据精度损失越大。   也就是说对于正常的zbuffer，采用非线性深度，在近平面处精度足够，但是远处容易产生z-flight， 并且znear越近，所有数据会越靠近0， 导致了较远处的z冲突更剧烈。这也就是为什么在解决z冲突时，尽量把znear离得远一些，效果会好一点

并且，由于float精度损失的原因，在远离0的地方精度损失更严重，加剧了这种现象。



可以通过反向存储深度z来改善情况，转换之后在1的地方精度越大，将float精度问题和非线性压缩中和一下，就能得到不错的效果。



## 2、model矩阵

如何确定缩放，平移，旋转的顺序

先缩放，再旋转，再位移。

位移一定是最后，否则旋转和缩放会对其产生影响。 缩放一定在旋转前，否则旋转后的再缩放比例关系不对。



## 3、投影矩阵

图像颠倒的问题，后来发现是投影矩阵的问题。  

正常规定znear和zfar都是负值，相机看向-z轴，推导出来的透视矩阵是没问题的，只是最后在ndc坐标下 z值，需要反一下才能用到深度缓冲中，因为需要确保z越小靠近屏幕越近，但是远平面的值会更小。

第二种改进方式是修改投影矩阵，传入的znear和zfar都是正数，重新推导矩阵，例如glm库函数一样，最后结果z就是深度值，w就是原view space下的深度值，都是正数， opengl中的实现方式就是这样，其实ndc坐标下已经变成了左手系。



